{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from urllib.request import urlopen\n",
    "import datetime\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import copy\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Crawl section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.livebitcoinnews.com/news/bitcoin-news/page/'\n",
    "result = []\n",
    "stop = False\n",
    "for i in range(1,100):\n",
    "    newurl = url+\"\"+str(i)\n",
    "    headers = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)' }\n",
    "    req = Request(url=newurl, headers=headers) \n",
    "    page = urlopen(req)\n",
    "    \n",
    "    soup = BeautifulSoup(page.read(), 'lxml', from_encoding=\"unicode\")\n",
    "    hrefs = SearchElement(\"h3\", \"title\", soup, soloElement=False, rawElement=True);\n",
    "    times = SearchElement(\"span\", \"time\", soup, soloElement=False, rawElement = False);\n",
    "\n",
    "    for href in hrefs:\n",
    "        res = href.find('a')['href']\n",
    "        if res == 'https://blockchain-life.com/europe/':\n",
    "            continue\n",
    "        result.append(res)\n",
    "        \n",
    "    for time in times:\n",
    "        print(time)\n",
    "        if \"February\" in time and \"2019\" in time:\n",
    "            print(\"I am stopping I found critical date\")          \n",
    "            stop = True\n",
    "            break\n",
    "    if stop:\n",
    "        break\n",
    "    sleep(0.3)\n",
    "\n",
    "urlsDf = pd.DataFrame(result, columns =['urls'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlsDf.to_csv(\"urltoLinks.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with soup\n",
    "soup.find_all('a', class_='featured-image')  \n",
    "href['href'] - where href is result of find('a')  \n",
    "soup.find(id=\"link3\")\n",
    "_tag.text - jadro tagu  \n",
    "_tag.next_sibling - text dalsieho tagu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckValidUrl(url):\n",
    "    if \"https://\" not in url:\n",
    "            print(\"I am skipping wrong html\")\n",
    "            return False\n",
    "    return True\n",
    "        \n",
    "def GetUrlSoup(url):\n",
    "    headers = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)' }\n",
    "    req = Request(url=url, headers=headers) \n",
    "    page = urlopen(req)\n",
    "    soup = BeautifulSoup(page.read(), 'lxml', from_encoding=\"unicode\")\n",
    "    return soup;\n",
    "\n",
    "def GetAuthorOfPublication(soup):\n",
    "    author = soup.find('span', class_=\"author\")\n",
    "    author = author.find('a').text \n",
    "    return author;\n",
    "\n",
    "def GetDateOfPublish(soup):\n",
    "    date = soup.find('span', class_=\"date\").text\n",
    "    date_time = datetime.datetime.strptime(date, '%B %d, %Y')\n",
    "    return date_time\n",
    "\n",
    "def GetTimeOfPublish(soup):\n",
    "    date = soup.find('span', class_=\"date\").text\n",
    "    time = soup.find('span', class_=\"time\").text.replace(\"Â·\", \"\").replace(\" \", \"\")\n",
    "    datetimeText = date+\" \"+time\n",
    "    date_time = datetime.datetime.strptime(datetimeText, '%B %d, %Y %I:%M%p')\n",
    "    return date_time\n",
    "\n",
    "def GetHeader(soup):\n",
    "    header = soup.find('h1', class_=\"title\")\n",
    "    header = header.find('a').text\n",
    "    return header.encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "\n",
    "def GetSubHeaders(soup):\n",
    "    subHeaders = []\n",
    "    divContent = soup.find('div', class_=\"post-content\")\n",
    "    headers = divContent.find_all('h2')\n",
    "    for header in headers:\n",
    "        subHeaders.append((header.text).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "    return subHeaders\n",
    "    \n",
    "def GetContent(soup):\n",
    "    TextContent = \"\"\n",
    "    divContent = soup.find('div', class_=\"post-content\")\n",
    "    textElements = divContent.find_all('p')\n",
    "    for p in textElements:\n",
    "        if \"You must be\" in p.text:\n",
    "            continue\n",
    "        TextContent = TextContent +\" \"+ p.text\n",
    "    return TextContent.encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    \n",
    "def GetTags(soup):\n",
    "    tags=[]\n",
    "    sectionTags = soup.find('span', class_=\"tags\")\n",
    "    stream = sectionTags.find_all('a')\n",
    "    for tag in stream:\n",
    "        tags.append((tag.text).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "    return tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultPd = pd.DataFrame(columns =['source','url','author','date','datetime','headertext','subheaders','contenttext','tags'])\n",
    "with open('urltoLinks.csv') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    for row in reader:\n",
    "        rowPd = pd.DataFrame(columns =['source','url','author','date','datetime','headertext','subheaders','contenttext','tags'])\n",
    "        crawlurl=row[0]\n",
    "        if not CheckValidUrl(crawlurl):\n",
    "            continue;\n",
    "        soup = GetUrlSoup(crawlurl)\n",
    "        \n",
    "        #content Crawl\n",
    "        Source = \"www.livebitcoinnews.com\";\n",
    "        Url = crawlurl;\n",
    "        Author = GetAuthorOfPublication(soup);\n",
    "        Date = GetDateOfPublish(soup);\n",
    "        DateTime = GetTimeOfPublish(soup);\n",
    "        HeaderText = GetHeader(soup);\n",
    "        SubHeadersText = GetSubHeaders(soup);\n",
    "        ContentText = GetContent(soup);\n",
    "        Tags = GetTags(soup);\n",
    "    \n",
    "    \n",
    "        rowPd.at[0,'source'] = Source\n",
    "        rowPd.at[0,'url'] = Url\n",
    "        rowPd.at[0,'author'] = Author\n",
    "        rowPd.at[0,'date'] = Date\n",
    "        rowPd.at[0,'datetime'] = DateTime\n",
    "        rowPd.at[0,'headertext'] = HeaderText\n",
    "        rowPd.at[0,'subheaders'] = SubHeadersText\n",
    "        rowPd.at[0,'contenttext'] = ContentText\n",
    "        rowPd.at[0,'tags'] = Tags\n",
    "        print(Date)\n",
    "        frames = [resultPd, rowPd]\n",
    "        resultPd = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultPd.to_csv(\"livebitcoinnews_data.csv\", encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
