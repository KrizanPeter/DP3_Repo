{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\kriza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kriza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kriza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kriza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\kriza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from urllib.request import urlopen\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "import copy\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "import string, re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYZER\n",
    "### POS Legend\n",
    "https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStopWords():\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    return stop_words\n",
    "\n",
    "def RemovePuncFromSentence(sentence):\n",
    "    sentence = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    return sentence\n",
    "\n",
    "def RemoveStopWords(stop_words, sentence):\n",
    "    return [w for w in sentence if not w in stop_words]\n",
    "\n",
    "\n",
    "def PrepareColumnForSentimentAnalysis(df, column):\n",
    "    #go through col and remove punc and create new array for future new col\n",
    "    newColName = \"cleaned_\"+column\n",
    "    stop_words = GetStopWords()\n",
    "    word_types = [\"J\",\"R\",\"V\", \"N\"]\n",
    "    \n",
    "    cleanedCol = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentences = sent_tokenize(row[column])\n",
    "        cleanSentences = [RemovePuncFromSentence(sentence) for sentence in sentences]\n",
    "        \n",
    "        #next step removing stop words\n",
    "        words = [word_tokenize(sentence) for sentence in cleanSentences]\n",
    "        filtered = [RemoveStopWords(stop_words, s) for s in words]\n",
    "        posWords = [nltk.pos_tag(tokenized_sent) for tokenized_sent in filtered]\n",
    "        cleanedSentence = []\n",
    "        for sent in posWords:\n",
    "            for w in sent:\n",
    "                if w[1][0] in word_types:\n",
    "                    cleanedSentence.append(w[0])\n",
    "                    \n",
    "        justString = \" \".join(str(x) for x in cleanedSentence)            \n",
    "        cleanedCol.append(justString)\n",
    "            \n",
    "    df[newColName] = cleanedCol\n",
    "    return df\n",
    "\n",
    "def GetSentimentScore(sentence):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return(str(score))\n",
    "    \n",
    "def GetSentimentOfColumn(df, column):\n",
    "    newCol = column\n",
    "    Sentiment_neg = []\n",
    "    Sentiment_pos = []\n",
    "    Sentiment_neut = []\n",
    "    Sentiment_comp = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "       \n",
    "        stringresult = GetSentimentScore(row[column])\n",
    "        stringresult = stringresult.replace('{', '').replace('}', '')\n",
    "        \n",
    "        Sentiment_neg.append(float((stringresult.split(',')[0]).split(':')[1]))\n",
    "        Sentiment_neut.append(float((stringresult.split(',')[1]).split(':')[1]))\n",
    "        Sentiment_pos.append(float((stringresult.split(',')[2]).split(':')[1]))\n",
    "        Sentiment_comp.append(float((stringresult.split(',')[3]).split(':')[1]))\n",
    "        \n",
    "    \n",
    "    df[newCol+\"_sent_neg\"] = Sentiment_neg  \n",
    "    df[newCol+\"_sent_neut\"] = Sentiment_neut  \n",
    "    df[newCol+\"_sent_pos\"] = Sentiment_pos  \n",
    "    df[newCol+\"_sent_comp\"] = Sentiment_comp  \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
